{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d90107e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{c}\n",
    "\\textbf{CAUSAL INFERENCE - Fall 2025}\\\\\\\\\n",
    "\\textit{Center for Data Science, New York University} \\\\\\\\\n",
    "\\textit{October 31, 2025}\\\\\\\\\\\n",
    "\\text{ Prepared by: Vivek Kumar Agarwal}\\\\\\\\\n",
    "\\textbf{Recitation 9: Control Variables and Matching}\\\\\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f3ab1e",
   "metadata": {},
   "source": [
    "\n",
    "![Causal Inference](../figures/CI_lab8_image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8892976",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2511bd0",
   "metadata": {},
   "source": [
    "## Today's Recitation \n",
    "\n",
    "- Quick Recap\n",
    "- Matching\n",
    "- Lets Code!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb687b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec53c70",
   "metadata": {},
   "source": [
    "## Part 1 - Quick Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3cbb27",
   "metadata": {},
   "source": [
    "## üéÉ The Halloween Brew: Quick Revision of Causal Inference Concepts\n",
    "\n",
    "### The Halloween Candy Mystery\n",
    "\n",
    "You notice that kids wearing **expensive, elaborate costumes** come home with **more candy** than kids in simple costumes. \n",
    "\n",
    "**Does this mean buying an expensive costume *causes* kids to get more candy?**\n",
    "\n",
    "**Let's analyse this using what we've learned until today**\n",
    "\n",
    "---\n",
    "\n",
    "### üßô‚Äç‚ôÄÔ∏è Concept 1: Association vs Causation\n",
    "\n",
    "**What we observe:**\n",
    "- Kids in elaborate costumes (store-bought, $50+) average 150 pieces of candy\n",
    "- Kids in simple costumes (homemade, $10) average 90 pieces of candy\n",
    "- Difference: 60 pieces\n",
    "\n",
    "**The question:** Does spending $40 more on a costume *cause* you to get 60 more pieces of candy?\n",
    "\n",
    "**Key insight:** Association does not imply causation!\n",
    "\n",
    "---\n",
    "\n",
    "### üëª Concept 2: Confounding Variables\n",
    "\n",
    "**The hidden factor:** Neighborhood wealth\n",
    "\n",
    "- Wealthy neighborhoods: Rich parents buy elaborate costumes AND these neighborhoods give out more candy\n",
    "- Less wealthy neighborhoods: Parents make simple costumes AND these neighborhoods give out less candy\n",
    "\n",
    "**The confounding structure:**\n",
    "- Neighborhood wealth ‚Üí Costume quality (parents can afford more)\n",
    "- Neighborhood wealth ‚Üí Candy haul (wealthier neighbors give more candy)\n",
    "\n",
    "**Result:** Costume quality and candy haul are *associated* because they share a common cause, not because one causes the other!\n",
    "\n",
    "---\n",
    "\n",
    "### ü¶á Concept 3: Control Variables\n",
    "\n",
    "**The solution:** Control for neighborhood wealth\n",
    "\n",
    "When we compare kids *within the same neighborhood*:\n",
    "- Kid with elaborate costume: 120 pieces\n",
    "- Kid with simple costume: 115 pieces  \n",
    "- True costume effect: Only 5 pieces!\n",
    "\n",
    "**What happened?**\n",
    "- **Naive comparison (60 pieces)** = True costume effect (5 pieces) + Selection bias (55 pieces from neighborhood differences)\n",
    "- By controlling for neighborhood, we remove the confounding and isolate the true causal effect\n",
    "\n",
    "**Key terms:**\n",
    "- **ATE (Average Treatment Effect):** Average effect across all kids if everyone switched costumes\n",
    "- **ATT (Average Treatment Effect on Treated):** Effect specifically for kids who actually wore elaborate costumes\n",
    "- **ATU (Average Treatment Effect on Untreated):** Effect for kids who wore simple costumes, if they had switched\n",
    "\n",
    "---\n",
    "\n",
    "### üï∑Ô∏è Concept 4: Natural Experiments\n",
    "\n",
    "**The problem recap:** We can't randomly assign costumes to kids (parents choose), so costume quality is confounded with neighborhood wealth.\n",
    "\n",
    "**A lucky break - The Costume Store Raffle:**\n",
    "\n",
    "Imagine costume store in every neighborhood held a raffle where they randomly gave away 50 elaborate costume vouchers:\n",
    "- Winners got $50 vouchers ‚Üí bought elaborate costumes\n",
    "- Non-winners ‚Üí stuck with simple costumes\n",
    "- **Crucially:** Winning the raffle was *random* - unrelated to neighborhood wealth, parent income, or anything else\n",
    "\n",
    "**Why this helps:**\n",
    "- The raffle creates random variation in costume quality\n",
    "- Winners and non-winners are similar on average (same neighborhoods, same wealth distribution)\n",
    "- Any difference in candy between raffle winners and losers must be due to the costume itself!\n",
    "- This is \"as good as random assignment\" - a **natural experiment**\n",
    "\n",
    "**Key insight:** When we find situations where treatment is assigned \"as if random,\" we can estimate causal effects without worrying about confounding!\n",
    "\n",
    "---\n",
    "\n",
    "### üéÉ Concept 5: Our Statistical Toolkit for Confounding\n",
    "\n",
    "Now we know *why* confounding is a problem. But how do we actually deal with it statistically?\n",
    "\n",
    "**Method 1: Regression with Control Variables**\n",
    "- Run regression: Candy = Œ≤‚ÇÄ + Œ≤‚ÇÅ(Elaborate Costume) + Œ≤‚ÇÇ(Neighborhood Wealth) + Œµ\n",
    "- Œ≤‚ÇÅ estimates the costume effect *holding neighborhood wealth constant*\n",
    "- This gives us the causal effect if we've controlled for all confounders\n",
    "\n",
    "**Method 2: Conditional ATE (Stratification)**\n",
    "- Calculate average candy for elaborate vs simple costumes *within each neighborhood*\n",
    "- Take weighted average across neighborhoods\n",
    "- Compares \"apples to apples\" - kids from same neighborhood\n",
    "\n",
    "**Method 3: Matching**\n",
    "- For each kid with elaborate costume, find a \"similar\" kid with simple costume (same neighborhood, same age, etc.)\n",
    "- Compare matched pairs\n",
    "- Coming up next!\n",
    "\n",
    "**Method 4: Natural Experiments**\n",
    "- Find situations with \"as-if random\" treatment assignment\n",
    "- Exploit this random variation to estimate causal effects\n",
    "- No need to control for confounders (they're balanced by randomization)\n",
    "\n",
    "**The common goal:** All these methods try to achieve the same thing - compare treated and control units that are similar in terms of confounders, so we can isolate the causal effect!\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"font-size: 3em;\">üéÉ</span>  **To Sum it All - The Big Picture**\n",
    "\n",
    "**Statistical reasoning steps:**\n",
    "\n",
    "1. **Observe a pattern** (elaborate costumes ‚Üí more candy)\n",
    "2. **Ask: Is this causal?** (or just correlation?)\n",
    "3. **Identify potential confounders** (neighborhood wealth)\n",
    "4. **Use statistical strategies** (control variables, natural experiments) to isolate causation\n",
    "5. **Estimate the true effect** (5 pieces, not 60!)\n",
    "\n",
    "Let us junp into details about: **Matching** - another tool for dealing with confounding!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbca1f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52824a7",
   "metadata": {},
   "source": [
    "## Part 2 - Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9870cd0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a505d0",
   "metadata": {},
   "source": [
    "## üéÉ Matching: Finding Your Candy-Collecting Twin\n",
    "\n",
    "### The Matching Idea\n",
    "\n",
    "**The problem we're solving:** Kids with elaborate costumes tend to come from wealthier neighborhoods. When we compare their candy hauls, we're comparing kids from different neighborhoods - not a fair comparison!\n",
    "\n",
    "**The matching solution:** For each kid with an elaborate costume, find a \"twin\" with a simple costume from the *same neighborhood* (and maybe same age, same start time, etc.). Compare these matched pairs.\n",
    "\n",
    "**Key insight:** By matching on confounders, we create treated and control groups that are similar in terms of the confounding variables. Any remaining difference must be due to the treatment itself!\n",
    "\n",
    "---\n",
    "\n",
    "### The Formal Setup\n",
    "\n",
    "**Notation:**\n",
    "- $Y$ = Candy collected\n",
    "- $S$ = Treatment (1 = elaborate costume, 0 = simple costume)\n",
    "- $C$ = Confounding variables (neighborhood wealth, age, etc.)\n",
    "- $U$ = Unobserved factors\n",
    "- $Y(1), Y(0)$ = Potential outcomes under treatment and control\n",
    "\n",
    "**Key assumption:** $S \\perp U \\mid C$\n",
    "\n",
    "This means: *Conditional on the observed confounders $C$, treatment is independent of unobserved factors.*\n",
    "\n",
    "In other words, once we account for neighborhood, age, etc., there's no remaining confounding!\n",
    "\n",
    "---\n",
    "\n",
    "### Why Matching Works: The Math\n",
    "\n",
    "**What we want to estimate:** The Average Treatment Effect (ATE)\n",
    "\n",
    "$$\n",
    "\\text{ATE} = \\mathbb{E}[Y(1) - Y(0)] = \\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)]\n",
    "$$\n",
    "\n",
    "**The key step:** Use the Law of Iterated Expectations to condition on $C$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[Y(1)] &= \\mathbb{E}[\\mathbb{E}[Y(1) \\mid C]] = \\sum_{c \\in C} \\mathbb{E}[Y(1) \\mid C=c] \\cdot \\mathbb{P}(C=c) \\\\\n",
    "\\mathbb{E}[Y(0)] &= \\mathbb{E}[\\mathbb{E}[Y(0) \\mid C]] = \\sum_{c \\in C} \\mathbb{E}[Y(0) \\mid C=c] \\cdot \\mathbb{P}(C=c)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Now use our assumption $S \\perp U \\mid C$:** This implies we can identify potential outcomes from observed data:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[Y(1) \\mid C=c] &= \\mathbb{E}[Y \\mid S=1, C=c] \\\\\n",
    "\\mathbb{E}[Y(0) \\mid C=c] &= \\mathbb{E}[Y \\mid S=0, C=c]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Putting it together:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{ATE} &= \\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)] \\\\\n",
    "&= \\sum_{c \\in C} \\mathbb{E}[Y(1) \\mid C=c] \\cdot \\mathbb{P}(C=c) - \\sum_{c \\in C} \\mathbb{E}[Y(0) \\mid C=c] \\cdot \\mathbb{P}(C=c) \\\\\n",
    "&= \\sum_{c \\in C} \\left( \\mathbb{E}[Y \\mid S=1, C=c] - \\mathbb{E}[Y \\mid S=0, C=c] \\right) \\cdot \\mathbb{P}(C=c)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is exactly our **matching estimator**!\n",
    "\n",
    "---\n",
    "\n",
    "### The Matching Estimator\n",
    "\n",
    "To obtain treated and control groups with similar covariate distributions, we use the matching estimator:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Matching ATE}\n",
    "& =  \\sum_{c \\in C} \\left( \\mathbb{E}[Y \\mid S=1, C = c] - \\mathbb{E}[Y \\mid S=0, C = c] \\right) \\cdot \\mathbb{P}(C = c)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**What this means:**\n",
    "- For each value of confounders $c$ (e.g., each neighborhood), calculate the treatment effect within that group\n",
    "- $\\mathbb{E}[Y \\mid S=1, C = c]$ = Average candy for elaborate costume kids in group $c$\n",
    "- $\\mathbb{E}[Y \\mid S=0, C = c]$ = Average candy for simple costume kids in group $c$\n",
    "- The difference gives us the treatment effect *within* that group\n",
    "- Weight each group's effect by how common that group is: $\\mathbb{P}(C = c)$\n",
    "- Sum across all groups to get the overall ATE\n",
    "\n",
    "**Why this works:** Within each matched group (same $c$), treated and control kids are comparable. We're doing \"apples-to-apples\" comparisons, then averaging across all groups!\n",
    "\n",
    "---\n",
    "\n",
    "### Regression vs Matching: Different Weighting Schemes\n",
    "\n",
    "**Important distinction:** Regression also estimates treatment effects by controlling for confounders, but it uses *different weights* than matching!\n",
    "\n",
    "**Regression estimator:**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha_1=\\sum_{k=1}^K \\mathbb{E}\\left[Y(S=1, U)-Y(S=0, U) \\mid \\mathbf{C}=\\mathbf{c}_k\\right] W\\left(\\mathbf{C}=\\mathbf{c}_k\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $W(\\mathbf{C}=\\mathbf{c}_k)$ is the weight of subgroup $k$.\n",
    "\n",
    "**Key difference:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W(C = c_k) \\neq \\mathbb{P}(C = c_k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**What this means:**\n",
    "- **Matching:** Weights each subgroup by its population probability $\\mathbb{P}(C = c_k)$\n",
    "- **Regression:** Uses weights $W(C = c_k)$ determined by the variance-covariance structure of the data\n",
    "- The regression weights depend on how the control variables vary in the sample, not just their frequencies\n",
    "\n",
    "**The implication:** Matching and regression can give different estimates of the ATE, even with the same data, because they weight subgroups differently!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07e2b1",
   "metadata": {},
   "source": [
    "### Matching in Practice: Exact vs Inexact Matching\n",
    "\n",
    "**The challenge:** Our mathematical derivation assumed we could find exact matches - kids with *exactly* the same neighborhood, age, start time, etc. But what if:\n",
    "- We have continuous confounders (e.g., neighborhood income is $45,231 vs $45,287)?\n",
    "- We have many confounders (neighborhood, age, height, parent education, etc.)?\n",
    "- No exact matches exist in our data?\n",
    "\n",
    "---\n",
    "\n",
    "### Exact Matching\n",
    "\n",
    "**What it is:** Find control units with *exactly* the same covariate values as treated units.\n",
    "\n",
    "**Example:** Match each kid with elaborate costume to a kid with simple costume from the exact same neighborhood.\n",
    "\n",
    "**Pros:**\n",
    "- Clean interpretation: Perfect \"apples-to-apples\" comparison within strata\n",
    "- The math we derived above applies directly\n",
    "\n",
    "**Cons:**\n",
    "- Often impossible with continuous variables or many covariates\n",
    "- Wastes data (many treated units can't be matched)\n",
    "- \"Curse of dimensionality\" - with 10 binary covariates, there are 1,024 possible combinations!\n",
    "\n",
    "---\n",
    "\n",
    "### The Dimension Problem\n",
    "\n",
    "**Example:** Suppose we want to match on:\n",
    "- Neighborhood (10 categories)\n",
    "- Age (5 categories)  \n",
    "- Start time (3 categories)\n",
    "- Parent education (4 categories)\n",
    "\n",
    "That's $10 \\times 5 \\times 3 \\times 4 = 600$ possible combinations! Most will have zero or very few observations.\n",
    "\n",
    "**The insight:** Maybe we don't need to match on *all* dimensions separately. What if we could summarize all confounders into a single score?\n",
    "\n",
    "---\n",
    "\n",
    "### Propensity Scores: Reducing Dimensions\n",
    "\n",
    "**What is a propensity score?**\n",
    "\n",
    "The propensity score is the **probability of receiving treatment given the confounders**:\n",
    "\n",
    "$$\n",
    "e(C) = P(S=1 \\mid C)\n",
    "$$\n",
    "\n",
    "**In our example:**\n",
    "$$\n",
    "e(C) = P(\\text{Elaborate Costume} = 1 \\mid \\text{Neighborhood, Age, Start Time, etc.})\n",
    "$$\n",
    "\n",
    "This is the probability that a kid wears an elaborate costume, given their characteristics.\n",
    "\n",
    "**Key property:** If $S \\perp U \\mid C$ (conditional independence), then $S \\perp U \\mid e(C)$ as well!\n",
    "\n",
    "**What this means:** Instead of matching on all confounders $C$, we can match on just the propensity score $e(C)$! This reduces many dimensions to just one number.\n",
    "\n",
    "---\n",
    "\n",
    "### Inexact Matching (Approximate Matching)\n",
    "\n",
    "**What it is:** When exact matches don't exist, find the \"closest\" control unit for each treated unit.\n",
    "\n",
    "**Common approaches:**\n",
    "\n",
    "**1. Nearest Neighbor Matching**\n",
    "   - For each treated unit, find the control unit with most similar covariates\n",
    "   - Measure similarity using distance metrics (Euclidean distance, Mahalanobis distance)\n",
    "   - Example: Match kids from neighborhoods with similar income levels (within $5,000)\n",
    "\n",
    "**2. Propensity Score Matching**\n",
    "   - Step 1: Estimate the propensity score $e(C) = P(S=1 \\mid C)$ using logistic regression\n",
    "   - Step 2: For each treated unit, find control unit(s) with similar propensity score\n",
    "   - Example: A kid with 70% probability of elaborate costume is matched with another kid who also has ~70% probability\n",
    "   - **Advantage:** Reduces all confounders to a single dimension - much easier to find matches!\n",
    "\n",
    "**3. Caliper Matching**\n",
    "   - Only match if control unit is within a specified distance (\"caliper\") of treated unit\n",
    "   - Can use calipers on propensity scores or individual covariates\n",
    "   - Discards treated units without close matches\n",
    "   - Example: Only match if propensity scores differ by less than 0.05\n",
    "\n",
    "**Trade-offs:**\n",
    "- **Bias vs Variance:** Close matches reduce bias but may increase variance (fewer matches)\n",
    "- **Match quality vs Sample size:** Strict matching criteria ‚Üí better matches but smaller sample\n",
    "\n",
    "---\n",
    "\n",
    "### The Bottom Line\n",
    "\n",
    "**Exact matching** gives us the cleanest causal estimates but is often infeasible.\n",
    "\n",
    "**Inexact matching** is practical but introduces approximation - we're not perfectly removing confounding, just reducing it.\n",
    "\n",
    "**Propensity scores** solve the dimensionality problem by summarizing all confounders into one number.\n",
    "\n",
    "**Regression** doesn't require finding individual matches but makes stronger functional form assumptions (linearity).\n",
    "\n",
    "All methods try to achieve the same goal: **compare similar units to isolate causal effects!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841a9924",
   "metadata": {},
   "source": [
    "## Let us Simulate our Halloween Candy Mystery!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd0aa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Simulating neighborhood wealth (the confounder)\n",
      "  - 387 kids in rich neighborhoods\n",
      "  - 613 kids in poor neighborhoods\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "üéÉ HALLOWEEN CANDY MATCHING DEMONSTRATION üéÉ\n",
    "\n",
    "Research Question: Does wearing an elaborate costume CAUSE kids to collect more candy?\n",
    "Or is the relationship confounded by neighborhood wealth?\n",
    "\n",
    "We'll use MATCHING to answer this question!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: SIMULATE THE DATA GENERATING PROCESS\n",
    "# ============================================================================\n",
    "# We know the TRUE causal structure because we're simulating it!\n",
    "# This lets us check if our methods recover the true effect.\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Simulate neighborhood wealth (the CONFOUNDER)\n",
    "# 40% of kids live in rich neighborhoods, 60% in poor neighborhoods\n",
    "neighborhood_wealth = np.random.binomial(1, 0.4, n)  # 1 = rich, 0 = poor\n",
    "\n",
    "print(\"Step 1: Simulating neighborhood wealth (the confounder)\")\n",
    "print(f\"  - {(neighborhood_wealth == 1).sum()} kids in rich neighborhoods\")\n",
    "print(f\"  - {(neighborhood_wealth == 0).sum()} kids in poor neighborhoods\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f86dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Simulating costume choice (confounded by neighborhood)\n",
      "  Rich neighborhoods: 262/387 elaborate (67.7%)\n",
      "  Poor neighborhoods: 182/613 elaborate (29.7%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: SIMULATE COSTUME CHOICE (THE TREATMENT)\n",
    "# ============================================================================\n",
    "# Costume choice depends on neighborhood wealth:\n",
    "#   - Rich neighborhood: 70% probability of elaborate costume\n",
    "#   - Poor neighborhood: 30% probability of elaborate costume\n",
    "# This creates CONFOUNDING!\n",
    "\n",
    "elaborate_costume = np.random.binomial(\n",
    "    1, \n",
    "    0.7 * neighborhood_wealth + 0.3 * (1 - neighborhood_wealth), \n",
    "    n\n",
    ")\n",
    "\n",
    "print(\"Step 2: Simulating costume choice (confounded by neighborhood)\")\n",
    "print(f\"  Rich neighborhoods: {(elaborate_costume[neighborhood_wealth==1] == 1).sum()}/{(neighborhood_wealth==1).sum()} elaborate ({(elaborate_costume[neighborhood_wealth==1] == 1).sum()/(neighborhood_wealth==1).sum()*100:.1f}%)\")\n",
    "print(f\"  Poor neighborhoods: {(elaborate_costume[neighborhood_wealth==0] == 1).sum()}/{(neighborhood_wealth==0).sum()} elaborate ({(elaborate_costume[neighborhood_wealth==0] == 1).sum()/(neighborhood_wealth==0).sum()*100:.1f}%)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad55d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Candy collection summary\n",
      "       Neighborhood_Wealth  Elaborate_Costume  Candy_Observe\n",
      "count          1000.000000        1000.000000    1000.000000\n",
      "mean              0.387000           0.444000      95.894278\n",
      "std               0.487307           0.497103      20.606588\n",
      "min               0.000000           0.000000      50.786495\n",
      "25%               0.000000           0.000000      79.718882\n",
      "50%               0.000000           0.000000      90.711038\n",
      "75%               1.000000           1.000000     114.538413\n",
      "max               1.000000           1.000000     151.377485\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: SIMULATE CANDY COLLECTION (THE OUTCOME)\n",
    "# ============================================================================\n",
    "# The TRUE data generating process:\n",
    "#   Base candy = 80 pieces\n",
    "#   + 5 pieces if elaborate costume (TRUE CAUSAL EFFECT)\n",
    "#   + 35 pieces if rich neighborhood (confounding effect)\n",
    "#   + random noise\n",
    "\n",
    "noise = np.random.normal(0, 10, n)\n",
    "\n",
    "# What we OBSERVE in the real world:\n",
    "candy_observe = 80 + 5 * elaborate_costume + 35 * neighborhood_wealth + noise\n",
    "\n",
    "# POTENTIAL OUTCOMES (counterfactuals we don't observe):\n",
    "# What if everyone wore elaborate costumes?\n",
    "candy_elaborate = 80 + 5 * 1 + 35 * neighborhood_wealth + noise\n",
    "\n",
    "# What if everyone wore simple costumes?\n",
    "# Note: Kids who chose elaborate are slightly more strategic (+3 bonus)\n",
    "candy_simple = 80 + 5 * 0 + 3 * elaborate_costume + 35 * neighborhood_wealth + noise\n",
    "\n",
    "# Put everything in a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Neighborhood_Wealth': neighborhood_wealth,\n",
    "    'Elaborate_Costume': elaborate_costume,\n",
    "    'Candy_Observe': candy_observe,\n",
    "    'Candy_Elaborate': candy_elaborate,\n",
    "    'Candy_Simple': candy_simple\n",
    "})\n",
    "\n",
    "print(\"Step 3: Candy collection summary\")\n",
    "print(df[['Neighborhood_Wealth', 'Elaborate_Costume', 'Candy_Observe']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e928746b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRUE TREATMENT EFFECTS (we know these because we simulated!)\n",
      "============================================================\n",
      "ATE (Average Treatment Effect):          3.67 pieces\n",
      "ATT (Effect on Treated):                 2.00 pieces\n",
      "ATU (Effect on Untreated):               5.00 pieces\n",
      "\n",
      "Notice: ATT > ATE > ATU\n",
      "Why? Kids who chose elaborate costumes are more strategic (+3 baseline)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: CALCULATE TRUE TREATMENT EFFECTS\n",
    "# ============================================================================\n",
    "# Since we simulated the data, we know the TRUE potential outcomes!\n",
    "# In reality, we never observe both Y(1) and Y(0) for the same person.\n",
    "\n",
    "# ATE: Average effect if we randomly assigned costumes to everyone\n",
    "ATE = df['Candy_Elaborate'].mean() - df['Candy_Simple'].mean()\n",
    "\n",
    "# ATT: Average effect for kids who actually wore elaborate costumes\n",
    "ATT = (df[df['Elaborate_Costume'] == 1]['Candy_Elaborate'].mean() - \n",
    "       df[df['Elaborate_Costume'] == 1]['Candy_Simple'].mean())\n",
    "\n",
    "# ATU: Average effect for kids who wore simple costumes\n",
    "ATU = (df[df['Elaborate_Costume'] == 0]['Candy_Elaborate'].mean() - \n",
    "       df[df['Elaborate_Costume'] == 0]['Candy_Simple'].mean())\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRUE TREATMENT EFFECTS (we know these because we simulated!)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ATE (Average Treatment Effect):          {ATE:.2f} pieces\")\n",
    "print(f\"ATT (Effect on Treated):                 {ATT:.2f} pieces\")\n",
    "print(f\"ATU (Effect on Untreated):               {ATU:.2f} pieces\")\n",
    "print()\n",
    "print(\"Notice: ATT > ATE > ATU\")\n",
    "print(\"Why? Kids who chose elaborate costumes are more strategic (+3 baseline)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a512bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NAIVE COMPARISON (without controlling for neighborhood)\n",
      "============================================================\n",
      "Average candy (elaborate costume):  106.15 pieces\n",
      "Average candy (simple costume):     87.70 pieces\n",
      "Naive difference:                   18.45 pieces\n",
      "\n",
      "TRUE ATE:                           3.67 pieces\n",
      "BIAS:                               14.79 pieces\n",
      "\n",
      "The naive approach MASSIVELY overstates the costume effect!\n",
      "Why? It conflates the costume effect with neighborhood wealth.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: THE NAIVE APPROACH (IGNORING CONFOUNDING)\n",
    "# ============================================================================\n",
    "# What if we just compare kids with elaborate vs simple costumes?\n",
    "# This is what we'd do if we ignored confounding!\n",
    "\n",
    "naive_effect = (df[df['Elaborate_Costume'] == 1]['Candy_Observe'].mean() - \n",
    "                df[df['Elaborate_Costume'] == 0]['Candy_Observe'].mean())\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"NAIVE COMPARISON (without controlling for neighborhood)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Average candy (elaborate costume):  {df[df['Elaborate_Costume']==1]['Candy_Observe'].mean():.2f} pieces\")\n",
    "print(f\"Average candy (simple costume):     {df[df['Elaborate_Costume']==0]['Candy_Observe'].mean():.2f} pieces\")\n",
    "print(f\"Naive difference:                   {naive_effect:.2f} pieces\")\n",
    "print()\n",
    "print(f\"TRUE ATE:                           {ATE:.2f} pieces\")\n",
    "print(f\"BIAS:                               {naive_effect - ATE:.2f} pieces\")\n",
    "print()\n",
    "print(\"The naive approach MASSIVELY overstates the costume effect!\")\n",
    "print(\"Why? It conflates the costume effect with neighborhood wealth.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80dcd017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MATCHING ESTIMATOR: Comparing within neighborhoods\n",
      "============================================================\n",
      "Effect in RICH neighborhoods:  6.11 pieces\n",
      "Effect in POOR neighborhoods:  5.62 pieces\n",
      "\n",
      "Proportion in poor neighborhoods: 0.613\n",
      "Proportion in rich neighborhoods: 0.387\n",
      "\n",
      "Matching ATE estimate:  5.81 pieces\n",
      "TRUE ATE:               3.67 pieces\n",
      "Estimation error:       2.14 pieces\n",
      "\n",
      " Matching successfully removes confounding bias\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: MATCHING ESTIMATOR\n",
    "# ============================================================================\n",
    "# Key idea: Compare kids WITHIN the same neighborhood (exact matching on C)\n",
    "# This creates \"apples-to-apples\" comparisons\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MATCHING ESTIMATOR: Comparing within neighborhoods\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate treatment effect in RICH neighborhoods\n",
    "ate_rich = (df[(df['Elaborate_Costume'] == 1) & (df['Neighborhood_Wealth'] == 1)]['Candy_Observe'].mean() - \n",
    "            df[(df['Elaborate_Costume'] == 0) & (df['Neighborhood_Wealth'] == 1)]['Candy_Observe'].mean())\n",
    "\n",
    "# Calculate treatment effect in POOR neighborhoods  \n",
    "ate_poor = (df[(df['Elaborate_Costume'] == 1) & (df['Neighborhood_Wealth'] == 0)]['Candy_Observe'].mean() - \n",
    "            df[(df['Elaborate_Costume'] == 0) & (df['Neighborhood_Wealth'] == 0)]['Candy_Observe'].mean())\n",
    "\n",
    "print(f\"Effect in RICH neighborhoods:  {ate_rich:.2f} pieces\")\n",
    "print(f\"Effect in POOR neighborhoods:  {ate_poor:.2f} pieces\")\n",
    "print()\n",
    "\n",
    "# Calculate population proportions P(C = c)\n",
    "proportions = df['Neighborhood_Wealth'].value_counts() / len(df)\n",
    "print(f\"Proportion in poor neighborhoods: {proportions[0]:.3f}\")\n",
    "print(f\"Proportion in rich neighborhoods: {proportions[1]:.3f}\")\n",
    "print()\n",
    "\n",
    "# Matching estimator: weighted average by population proportions\n",
    "# ATE_matching = Œ£ [E[Y|S=1,C=c] - E[Y|S=0,C=c]] * P(C=c)\n",
    "ate_matching = ate_rich * proportions[1] + ate_poor * proportions[0]\n",
    "\n",
    "print(f\"Matching ATE estimate:  {ate_matching:.2f} pieces\")\n",
    "print(f\"TRUE ATE:               {ATE:.2f} pieces\")\n",
    "print(f\"Estimation error:       {abs(ate_matching - ATE):.2f} pieces\")\n",
    "print()\n",
    "print(\" Matching successfully removes confounding bias\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e44d8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VARIANCE OF TREATMENT (matters for regression weights)\n",
      "============================================================\n",
      "Var(Elaborate_Costume | Poor neighborhood):  0.2091\n",
      "Var(Elaborate_Costume | Rich neighborhood):  0.2192\n",
      "\n",
      "Regression will weight neighborhoods based on:\n",
      "  1. Sample size in each group\n",
      "  2. Variance of treatment in each group\n",
      "This is NOT the same as the population proportion P(C=c)!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: UNDERSTANDING REGRESSION WEIGHTS\n",
    "# ============================================================================\n",
    "# Regression uses DIFFERENT weights than matching!\n",
    "# Let's see the variance of treatment in each neighborhood\n",
    "\n",
    "var_poor = df[df['Neighborhood_Wealth'] == 0]['Elaborate_Costume'].var()\n",
    "var_rich = df[df['Neighborhood_Wealth'] == 1]['Elaborate_Costume'].var()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VARIANCE OF TREATMENT (matters for regression weights)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Var(Elaborate_Costume | Poor neighborhood):  {var_poor:.4f}\")\n",
    "print(f\"Var(Elaborate_Costume | Rich neighborhood):  {var_rich:.4f}\")\n",
    "print()\n",
    "print(\"Regression will weight neighborhoods based on:\")\n",
    "print(\"  1. Sample size in each group\")\n",
    "print(\"  2. Variance of treatment in each group\")\n",
    "print(\"This is NOT the same as the population proportion P(C=c)!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e757583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REGRESSION ESTIMATOR\n",
      "============================================================\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          Candy_Observe   R-squared:                       0.775\n",
      "Model:                            OLS   Adj. R-squared:                  0.775\n",
      "Method:                 Least Squares   F-statistic:                     1717.\n",
      "Date:                Fri, 31 Oct 2025   Prob (F-statistic):          9.88e-324\n",
      "Time:                        11:24:41   Log-Likelihood:                -3698.2\n",
      "No. Observations:                1000   AIC:                             7402.\n",
      "Df Residuals:                     997   BIC:                             7417.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "Intercept              79.9201      0.443    180.588      0.000      79.052      80.789\n",
      "Elaborate_Costume       5.8117      0.671      8.661      0.000       4.495       7.129\n",
      "Neighborhood_Wealth    34.6092      0.685     50.559      0.000      33.266      35.952\n",
      "==============================================================================\n",
      "Omnibus:                        1.170   Durbin-Watson:                   2.050\n",
      "Prob(Omnibus):                  0.557   Jarque-Bera (JB):                1.054\n",
      "Skew:                           0.025   Prob(JB):                        0.590\n",
      "Kurtosis:                       3.151   Cond. No.                         3.09\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: REGRESSION ESTIMATOR\n",
    "# ============================================================================\n",
    "# Run OLS regression controlling for neighborhood wealth\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REGRESSION ESTIMATOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ols = sm.ols(formula='Candy_Observe ~ Elaborate_Costume + Neighborhood_Wealth', data=df).fit()\n",
    "print(ols.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e0de39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL COMPARISON: Matching vs Regression\n",
      "============================================================\n",
      "TRUE ATE:                    3.67 pieces\n",
      "Matching estimate:           5.81 pieces\n",
      "Regression estimate:         5.81 pieces\n",
      "\n",
      "Matching error:              2.14 pieces\n",
      "Regression error:            2.14 pieces\n",
      "\n",
      "Key insight: Both methods control for confounding, but they\n",
      "weight subgroups differently:\n",
      "  - Matching weights by P(C=c): 0.613 and 0.387\n",
      "  - Regression weights by variance-covariance structure\n",
      "\n",
      "Both are valid! Which to use depends on your target estimand.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: COMPARING MATCHING VS REGRESSION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL COMPARISON: Matching vs Regression\")\n",
    "print(\"=\"*60)\n",
    "print(f\"TRUE ATE:                    {ATE:.2f} pieces\")\n",
    "print(f\"Matching estimate:           {ate_matching:.2f} pieces\")\n",
    "print(f\"Regression estimate:         {ols.params['Elaborate_Costume']:.2f} pieces\")\n",
    "print()\n",
    "print(f\"Matching error:              {abs(ate_matching - ATE):.2f} pieces\")\n",
    "print(f\"Regression error:            {abs(ols.params['Elaborate_Costume'] - ATE):.2f} pieces\")\n",
    "print()\n",
    "print(\"Key insight: Both methods control for confounding, but they\")\n",
    "print(\"weight subgroups differently:\")\n",
    "print(f\"  - Matching weights by P(C=c): {proportions[0]:.3f} and {proportions[1]:.3f}\")\n",
    "print(\"  - Regression weights by variance-covariance structure\")\n",
    "print()\n",
    "print(\"Both are valid! Which to use depends on your target estimand.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c957437",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03d081",
   "metadata": {},
   "source": [
    "### When to Use Matching vs Regression?\n",
    "\n",
    "**The key difference:** Both methods control for confounding, but they weight subgroups differently.\n",
    "\n",
    "- **Matching:** Weights by population proportions $\\mathbb{P}(C=c)$\n",
    "- **Regression:** Weights by variance-covariance structure (where you have more statistical information)\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Matching\n",
    "\n",
    "**Use matching when you want the population average effect or care about representativeness:**\n",
    "\n",
    "- **Population policy questions:** \"What's the effect for a randomly selected person?\" Match weights subgroups by their actual prevalence in the population\n",
    "- **Few discrete confounders:** With 2-3 categorical variables (like neighborhood type), exact matching is feasible and intuitive\n",
    "- **Distrust linearity:** When effects might vary drastically across subgroups and you don't want to assume a linear relationship\n",
    "- **Subgroup analysis:** Want to see effects separately for each subgroup before combining\n",
    "\n",
    "**Halloween example:** If 60% of kids are from poor neighborhoods and 40% from rich neighborhoods, matching gives you an effect that represents this actual distribution. This answers: \"What happens to a typical trick-or-treater?\"\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Regression\n",
    "\n",
    "**Use regression when you have many confounders or need efficiency:**\n",
    "\n",
    "- **Many continuous confounders:** With 5+ variables (age, height, income, distance, time), exact matching becomes impossible due to curse of dimensionality\n",
    "- **Multiple controls:** Easy to include many control variables without combinatorial explosion\n",
    "- **Interaction effects:** Natural framework for testing if effects vary across subgroups (e.g., costume effect differs by age)\n",
    "\n",
    "**Halloween example:** If you need to control for neighborhood wealth + age + start time + group size + walking speed, regression handles all these simultaneously without needing to find exact matches.\n",
    "\n",
    "---\n",
    "\n",
    "### In Our Halloween Case?\n",
    "\n",
    "With only one binary confounder (neighborhood wealth), both methods work well!\n",
    "\n",
    "- **For \"typical trick-or-treater\" effect:** Use matching (weights 60% poor, 40% rich)\n",
    "- **To examine where costume matters most:** Use matching to compare effects within each neighborhood separately\n",
    "- **If we had 10+ confounders:** Switch to regression or propensity score matching\n",
    "\n",
    "**Bottom line:** Choose based on research question and data structure, not because one is \"better\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c2051c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c115a",
   "metadata": {},
   "source": [
    "## Why Does Regression Weight by Treatment Variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80c4bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6cbaf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's see the mathematical reason why regression gives different weights than matching. We'll work through the linear algebra to understand what's happening under the hood.\n",
    "\n",
    "---\n",
    "\n",
    "### The Setup: Two Subgroups\n",
    "\n",
    "Suppose we have two subgroups (like our rich and poor neighborhoods). We're estimating the regression:\n",
    "\n",
    "$$\n",
    "Y = X \\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "where $X$ is our treatment variable (elaborate costume) and $Y$ is the outcome (candy collected).\n",
    "\n",
    "We can partition our data by subgroup:\n",
    "\n",
    "$$\n",
    "X=\\left[\\begin{array}{l}\n",
    "X_1 \\\\\n",
    "X_2\n",
    "\\end{array}\\right] \\quad \\text{and} \\quad Y=\\left[\\begin{array}{l}\n",
    "Y_1 \\\\\n",
    "Y_2\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### The OLS Estimator for Pooled Data\n",
    "\n",
    "The standard OLS formula gives us:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{pooled}} = (X^T X)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "Breaking this into subgroups:\n",
    "\n",
    "$$\n",
    "X^T X = X_1^T X_1 + X_2^T X_2 \\quad \\text{and} \\quad X^T Y = X_1^T Y_1 + X_2^T Y_2\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{pooled}} = \\left(X_1^T X_1 + X_2^T X_2\\right)^{-1}\\left(X_1^T Y_1 + X_2^T Y_2\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Rewriting as a Weighted Average\n",
    "\n",
    "Now, let's define the **within-subgroup estimates**:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\left(X_1^T X_1\\right)^{-1} X_1^T Y_1 \\quad \\text{and} \\quad \\hat{\\beta}_2 = \\left(X_2^T X_2\\right)^{-1} X_2^T Y_2\n",
    "$$\n",
    "\n",
    "These are what we'd get if we ran regression separately in each subgroup.\n",
    "\n",
    "We can rewrite the pooled estimator by substituting back:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{pooled}} = \\left(X_1^T X_1 + X_2^T X_2\\right)^{-1}\\left(X_1^T X_1 \\hat{\\beta}_1 + X_2^T X_2 \\hat{\\beta}_2\\right)\n",
    "$$\n",
    "\n",
    "This shows that the pooled estimate is a **weighted average** of subgroup estimates:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{\\text{pooled}} = W_1 \\hat{\\beta}_1 + W_2 \\hat{\\beta}_2\n",
    "$$\n",
    "\n",
    "where the weights are:\n",
    "\n",
    "$$\n",
    "W_1 = \\left(X_1^T X_1 + X_2^T X_2\\right)^{-1} X_1^T X_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_2 = \\left(X_1^T X_1 + X_2^T X_2\\right)^{-1} X_2^T X_2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### The Key Insight: $X^T X$ Relates to Treatment Variance\n",
    "\n",
    "**What is $X^T X$?** For our treatment variable (after demeaning), $X^T X \\propto n \\cdot \\text{Var}(X)$ where $n$ is sample size.\n",
    "\n",
    "Therefore:\n",
    "- $X_1^T X_1 \\propto n_1 \\cdot \\text{Var}(X_1)$ (treatment variance in subgroup 1)\n",
    "- $X_2^T X_2 \\propto n_2 \\cdot \\text{Var}(X_2)$ (treatment variance in subgroup 2)\n",
    "\n",
    "**The implication:** Subgroups with **larger treatment variance** get **larger weight** in the regression estimate!\n",
    "\n",
    "**Why this makes sense statistically:** Subgroups with more variation in treatment provide more information for estimating the treatment effect, so regression naturally gives them more weight.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50da62",
   "metadata": {},
   "source": [
    "## Next Recitation \n",
    "\n",
    "+ Explore more about Matching Instruments \n",
    "+ Instrumental variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a8625",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roi_segment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
